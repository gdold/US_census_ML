{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import r_, c_ # Convenient syntax to create numpy arrays with e.g. r_[1,2,3] (concatenates in a row) or c_[1,2,3] (concatenates in a column)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using these scikit-learn modules\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We'll import the models themselves as and when we use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import census data\n",
    "Load the CSVs as Pandas DataFrames, and the columns into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metadata file to create list of column names\n",
    "columns = []\n",
    "for line in open('us_census_full/census_income_metadata.txt','r'):\n",
    "    li=line.strip() # Strip trailing whitespace\n",
    "    if not (li.startswith(\"|\") or li.startswith(\"-\") or li==''): # Strip comment lines and empty lines\n",
    "        variable = li.split(':') # Split column description at colon\n",
    "        columns.append(variable[0]) # Add column name to list\n",
    "\n",
    "# Add final (problem) column name\n",
    "columns.append('Income over 50k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US census data for training and validation\n",
    "# Separator is a comma followed by a space\n",
    "# NaN values are specified by '?'\n",
    "census_learn = pd.read_csv('us_census_full/census_income_learn.csv', names = columns, sep=',\\s', na_values=['?'], engine='python')\n",
    "census_learn['Income over 50k'] = census_learn['Income over 50k']=='50000+.' # Convert last (problem) column to bool\n",
    "census_learn.drop('instance weight', axis=1, inplace=True) # Drop instance weight column as not to be used in classifier\n",
    "#census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US census data for test\n",
    "# Separator is a comma followed by a space\n",
    "# NaN values are specified by '?'\n",
    "census_test = pd.read_csv('us_census_full/census_income_test.csv', names = columns, sep=',\\s', na_values=['?'], engine='python')\n",
    "census_test['Income over 50k'] = census_test['Income over 50k']=='50000+.' # Convert last (problem) column to bool\n",
    "census_test.drop('instance weight', axis=1, inplace=True) # Drop instance weight column as not to be used in classifier\n",
    "#census_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and transform features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's split off the income column into another dataset y to keep it separate from the variables X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census_learn\n",
    "income = census['Income over 50k']\n",
    "census = census.drop('Income over 50k', axis=1) # Drop income data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census data contains missing values. As a basic method of dealing with these we'll drop them. An alternative would be to impute (i.e. infer from other data) their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_no_missing_data = census.notna().all().tolist() # Keep for later test data\n",
    "census = census.loc[:,columns_with_no_missing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some features that are unlikely to be correlated to income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.drop(['year'],axis=1) # The year the census entry was completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try adding some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_educated_degrees = ['Bachelors degree(BA AB BS)','Masters degree(MA MS MEng MEd MSW MBA)','Associates degree-academic program','Associates degree-occup /vocational','Prof school degree (MD DDS DVM LLB JD)','Doctorate degree(PhD EdD)']\n",
    "census['highly educated'] = census['education'].isin(highly_educated_degrees).astype(int) # Does this individual have a Bachelors degree or higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, *sex* is either Male or Female. To avoid creating two colinear columns when we transform the data, we'll instead change this to \"is male\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['is male'] = (census['sex'] == 'Male').astype(int)\n",
    "census = census.drop('sex',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of categorical data in our census. We can preprocess these into numerical values so logistic regression can function. We'll use a one-hot encoder so that each category becomes a feature on its own, to avoid ordering features in a manner that is unphysical.\n",
    "\n",
    "In order to understand the model, we want to be able to identify the most important features.\n",
    "\n",
    "To do this, we must encode the categorical and continuous features separately so that we can call categoricalTransformer.get_feature_names() as this method does not support passthrough. The two feature sets can then be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First transform the categorical features using a one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features, which will be tranformed into numerical values\n",
    "categorical_feature_mask = census.dtypes==object\n",
    "categorical_columns = census.columns[categorical_feature_mask].tolist() # List of categorical columns\n",
    "# Add to the list some columns that are also categorical, despite their values being numeric\n",
    "categorical_columns.extend(['detailed industry recode','detailed occupation recode','own business or self employed']) \n",
    "\n",
    "# Fit a One-Hot Encoder to each categorical column\n",
    "# This turns each category of the categorical features into its very own feature\n",
    "# Which prevents adding an unphysical ordering to features as an ordinal encoder would\n",
    "categoricalTransformer = ColumnTransformer([('encoder', preprocessing.OneHotEncoder(), categorical_columns)], remainder='drop')\n",
    "categoricalTransformer.fit(census) # Calculate the transformation\n",
    "categorical_feature_names = categoricalTransformer.get_feature_names() # Follow the feature names through the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass through the numerical features unmodified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical features, which we will pass straight through the transformation\n",
    "noncategorical_feature_mask = census.dtypes!=object\n",
    "noncategorical_columns = census.columns[noncategorical_feature_mask].tolist() # List of noncategorical (continuous) columns\n",
    "# Remove columns that are are categorical despite containing numbers\n",
    "noncategorical_columns.remove('detailed industry recode');noncategorical_columns.remove('detailed occupation recode');noncategorical_columns.remove('own business or self employed') \n",
    "\n",
    "# Pass the continuous features through the transformation\n",
    "noncategoricalTransformer = ColumnTransformer([('encoder', 'passthrough', noncategorical_columns)], remainder='drop')\n",
    "noncategoricalTransformer.fit(census) # Calculate the transformation\n",
    "noncategorical_feature_names = noncategorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine these two transformations to create the transformed feature data X and the target data y. Also keep track of the feature names for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the features\n",
    "X_categorical = categoricalTransformer.transform(census).toarray()\n",
    "X_noncategorical = noncategoricalTransformer.transform(census)\n",
    "X = np.c_[X_categorical,X_noncategorical] # Combine the two feature sets columnwise\n",
    "y = income\n",
    "\n",
    "# Combine the feature names for later analysis\n",
    "feature_names = r_[categorical_feature_names + noncategorical_feature_names]\n",
    "census_transformed = pd.DataFrame(X,columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale data (zero mean and unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaleTransformer = preprocessing.StandardScaler()\n",
    "scaleTransformer.fit(X)\n",
    "X = scaleTransformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the learing data into a training set (70%) and a cross-validation set (30%) so the accuracy of each model can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train logistic regression\n",
    "\n",
    "There are several options and hyperparameters we can play with here. In the first instance let's try scikit-learn's defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score = 95.31%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and fit logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=500) # Optional: class_weight = 'balanced'\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "# Score using the training data\n",
    "training_score = log_reg.score(X_train,y_train)\n",
    "print('Training score = {:2.2f}%'.format(training_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance with cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score = 95.26%\n",
      "       Precision = 73.82%\n",
      "          Recall = 40.63%\n",
      "[[55459   554]\n",
      " [ 2282  1562]]\n"
     ]
    }
   ],
   "source": [
    "score = log_reg.score(X_val,y_val)\n",
    "confusion = metrics.confusion_matrix(y_val,log_reg.predict(X_val))\n",
    "precision = confusion[1,1] / (confusion[1,1] + confusion[0,1]) # TP / (TP + FP)\n",
    "recall = confusion[1,1] / (confusion[1,1] + confusion[1,0]) # TP / (TP + FN)\n",
    "\n",
    "print('Validation score = {:2.2f}%'.format(score*100))\n",
    "print('       Precision = {:2.2f}%'.format(precision*100)) # Accuracy of positive predictions, i.e. how many of the predicted positives were accurate\n",
    "print('          Recall = {:2.2f}%'.format(recall*100)) # True positive rate, i.e. how many of the true positives were found\n",
    "print(confusion) # Confusion matrix [[TN,FP],[FN,TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try giving more weight to positive training data, which is sometimes beneficial if the set of positive results is small (as it is here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score = 85.50%\n"
     ]
    }
   ],
   "source": [
    "# Create and fit logistic regression model\n",
    "log_reg_balanced = LogisticRegression(max_iter=500, class_weight = 'balanced')\n",
    "log_reg_balanced.fit(X_train,y_train)\n",
    "\n",
    "# Score using the training data\n",
    "training_score = log_reg_balanced.score(X_train,y_train)\n",
    "print('Training score = {:2.2f}%'.format(training_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance with cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score = 85.62%\n",
      "       Precision = 29.60%\n",
      "          Recall = 89.91%\n",
      "[[47793  8220]\n",
      " [  388  3456]]\n"
     ]
    }
   ],
   "source": [
    "score = log_reg_balanced.score(X_val,y_val)\n",
    "confusion = metrics.confusion_matrix(y_val,log_reg_balanced.predict(X_val))\n",
    "precision = confusion[1,1] / (confusion[1,1] + confusion[0,1]) # TP / (TP + FP)\n",
    "recall = confusion[1,1] / (confusion[1,1] + confusion[1,0]) # TP / (TP + FN)\n",
    "\n",
    "print('Validation score = {:2.2f}%'.format(score*100))\n",
    "print('       Precision = {:2.2f}%'.format(precision*100)) # Accuracy of positive predictions, i.e. how many of the predicted positives were accurate\n",
    "print('          Recall = {:2.2f}%'.format(recall*100)) # True positive rate, i.e. how many of the true positives were found\n",
    "print(confusion) # Confusion matrix [[TN,FP],[FN,TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the classes resulted in fewer false negatives but far more false positives, with a lower overall score.\n",
    "\n",
    "There are several other options and parameters I have tried modifying, including the solvers, and types and strengths of regularisation, but these are the best I got through logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score = 95.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth=10, random_state=9354)\n",
    "decision_tree.fit(X_train,y_train)\n",
    "\n",
    "# Score using the training data\n",
    "training_score = decision_tree.score(X_train,y_train)\n",
    "print('Training score = {:2.2f}%'.format(training_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score = 95.12%\n",
      "       Precision = 72.90%\n",
      "          Recall = 38.22%\n",
      "[[55467   546]\n",
      " [ 2375  1469]]\n"
     ]
    }
   ],
   "source": [
    "score = decision_tree.score(X_val,y_val)\n",
    "confusion = metrics.confusion_matrix(y_val,decision_tree.predict(X_val))\n",
    "precision = confusion[1,1] / (confusion[1,1] + confusion[0,1]) # TP / (TP + FP)\n",
    "recall = confusion[1,1] / (confusion[1,1] + confusion[1,0]) # TP / (TP + FN)\n",
    "\n",
    "print('Validation score = {:2.2f}%'.format(score*100))\n",
    "print('       Precision = {:2.2f}%'.format(precision*100)) # Accuracy of positive predictions\n",
    "print('          Recall = {:2.2f}%'.format(recall*100)) # True positive rate\n",
    "print(confusion) # Confusion matrix [[TN,FP],[FN,TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree performed strictly worse than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "A random forest classifier trains a number of decision trees with a random seed, and returns their average prediction (in the case of classification, the mode result)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score = 99.94%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=1357, n_estimators=200, n_jobs=-1)\n",
    "random_forest.fit(X_train,y_train)\n",
    "\n",
    "# Score using the training data\n",
    "training_score = random_forest.score(X_train,y_train)\n",
    "print('Training score = {:2.2f}%'.format(training_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score = 95.37%\n",
      "       Precision = 76.02%\n",
      "          Recall = 40.82%\n",
      "[[55518   495]\n",
      " [ 2275  1569]]\n"
     ]
    }
   ],
   "source": [
    "score = random_forest.score(X_val,y_val)\n",
    "confusion = metrics.confusion_matrix(y_val,random_forest.predict(X_val))\n",
    "precision = confusion[1,1] / (confusion[1,1] + confusion[0,1]) # TP / (TP + FP)\n",
    "recall = confusion[1,1] / (confusion[1,1] + confusion[1,0]) # TP / (TP + FN)\n",
    "\n",
    "print('Validation score = {:2.2f}%'.format(score*100))\n",
    "print('       Precision = {:2.2f}%'.format(precision*100)) # Accuracy of positive predictions\n",
    "print('          Recall = {:2.2f}%'.format(recall*100)) # True positive rate\n",
    "print(confusion) # Confusion matrix [[TN,FP],[FN,TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest gives slightly better false positive and false negative rates than logistic regression, and a slightly higher overall score.\n",
    "\n",
    "**We'll use this as our final model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other classifiers\n",
    "Scikit-learn offers other classification models, of which I also tried an AdaBoost classifier, Multi-layer Perceptron classifier, and Quadratic discriminant analysis.\n",
    "These performed worse than than logistic regression and random forest models with the default settings, and I did not spend the time attempting to optimise these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Let's test our model's real performance with data we haven't trained or validated with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply same transformations as with training data. (Ideally this would be using a pipeline, but this turned out to be complicated - see discussion section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census_test\n",
    "income = census['Income over 50k']\n",
    "census = census.drop('Income over 50k', axis=1) # Drop income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.loc[:,columns_with_no_missing_data] # Drop columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.drop(['year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_educated_degrees = ['Bachelors degree(BA AB BS)','Masters degree(MA MS MEng MEd MSW MBA)','Associates degree-academic program','Associates degree-occup /vocational','Prof school degree (MD DDS DVM LLB JD)','Doctorate degree(PhD EdD)']\n",
    "census['highly educated'] = census['education'].isin(highly_educated_degrees).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['is male'] = (census['sex'] == 'Male').astype(int)\n",
    "census = census.drop('sex',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the features\n",
    "X_categorical = categoricalTransformer.transform(census).toarray()\n",
    "X_noncategorical = noncategoricalTransformer.transform(census)\n",
    "X_test = np.c_[X_categorical,X_noncategorical] # Combine the two feature sets columnwise\n",
    "y_test = income\n",
    "\n",
    "X_test = scaleTransformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score the model on the test data. Best performing model according to cross-validation was the **random forest**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score = 95.39%\n",
      " Precision = 73.49%\n",
      "    Recall = 40.06%\n",
      "[[92682   894]\n",
      " [ 3708  2478]]\n"
     ]
    }
   ],
   "source": [
    "score = final_model.score(X_test,y_test)\n",
    "confusion = metrics.confusion_matrix(y_test,final_model.predict(X_test))\n",
    "precision = confusion[1,1] / (confusion[1,1] + confusion[0,1]) # TP / (TP + FP)\n",
    "recall = confusion[1,1] / (confusion[1,1] + confusion[1,0]) # TP / (TP + FN)\n",
    "\n",
    "print('Test score = {:2.2f}%'.format(score*100))\n",
    "print(' Precision = {:2.2f}%'.format(precision*100)) # Accuracy of positive predictions\n",
    "print('    Recall = {:2.2f}%'.format(recall*100)) # True positive rate\n",
    "print(confusion) # Confusion matrix [[TN,FP],[FN,TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insigts and analysis of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these models we can determine what features in the US census dataset are correlated with high-earners. \n",
    "\n",
    "To find out these features, we investigate which features the model deemed important, rank them by their coefficient, and lookup which features they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features picked out by model <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0.09885442762707386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dividends from stocks</td>\n",
       "      <td>0.07501369317765014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capital gains</td>\n",
       "      <td>0.06406771209696899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num persons worked for employer</td>\n",
       "      <td>0.043058810851889125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>highly educated</td>\n",
       "      <td>0.03238181643999346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weeks worked in year</td>\n",
       "      <td>0.030622688671986146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is male</td>\n",
       "      <td>0.02629339848752002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>capital losses</td>\n",
       "      <td>0.02133598669847576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encoder__x5_Executive admin and managerial</td>\n",
       "      <td>0.016356786012077237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encoder__x19_2</td>\n",
       "      <td>0.015280810431397734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      feature            importance\n",
       "0                                         age   0.09885442762707386\n",
       "1                       dividends from stocks   0.07501369317765014\n",
       "2                               capital gains   0.06406771209696899\n",
       "3             num persons worked for employer  0.043058810851889125\n",
       "4                             highly educated   0.03238181643999346\n",
       "5                        weeks worked in year  0.030622688671986146\n",
       "6                                     is male   0.02629339848752002\n",
       "7                              capital losses   0.02133598669847576\n",
       "8  encoder__x5_Executive admin and managerial  0.016356786012077237\n",
       "9                              encoder__x19_2  0.015280810431397734"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features_arg = np.flipud(np.argsort(np.abs(final_model.feature_importances_))[-10:]) # 10 most important features by coefficient\n",
    "important_features_coef = final_model.feature_importances_[important_features_arg] # Their coefficients\n",
    "important_features = pd.DataFrame(c_[feature_names[important_features_arg],important_features_coef],columns=['feature','importance']) # Format it nicely\n",
    "print('Features picked out by model {:s}'.format(str(final_model.__class__)))\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest picked out several important features. A quick summary:\n",
    "1. High earners tend to be older (age)\n",
    "2. Returns on investments are correlated with high earnings (dividends from stocks, capital gains)\n",
    "3. High earners tend to work for very large companies (num persons worked for employer)\n",
    "4. Being highly educated (at least a Bachelors degree) increases earnings (highly educated)\n",
    "5. Working more weeks in a year is indicative of higher earning (weeks worked in year)\n",
    "6. Male privilege is real (is male)\n",
    "7. Executive positions are correlated with high earnings (Executive admin and managerial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downside of the analysis above is that the *importance* returned by a random forest does not indicate a positive or negative correlation, so some common sense was needed in interpreting its results.\n",
    "The logistic regression has an advantage over the random forest for this, as its coefficients can be positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features picked out by model <class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weeks worked in year</td>\n",
       "      <td>0.896624387957522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>0.7393690418987341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encoder__x10_Nonfiler</td>\n",
       "      <td>-0.5807120385187193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is male</td>\n",
       "      <td>0.5624864818998175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capital gains</td>\n",
       "      <td>0.4604130595247235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encoder__x10_Joint both under 65</td>\n",
       "      <td>0.4342619002298662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num persons worked for employer</td>\n",
       "      <td>0.38482260438650867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>encoder__x15_Mother only present</td>\n",
       "      <td>-0.37962440121208363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dividends from stocks</td>\n",
       "      <td>0.34578452894087164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encoder__x1_Children</td>\n",
       "      <td>-0.34345597567682534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            feature           coefficient\n",
       "0              weeks worked in year     0.896624387957522\n",
       "1                               age    0.7393690418987341\n",
       "2             encoder__x10_Nonfiler   -0.5807120385187193\n",
       "3                           is male    0.5624864818998175\n",
       "4                     capital gains    0.4604130595247235\n",
       "5  encoder__x10_Joint both under 65    0.4342619002298662\n",
       "6   num persons worked for employer   0.38482260438650867\n",
       "7  encoder__x15_Mother only present  -0.37962440121208363\n",
       "8             dividends from stocks   0.34578452894087164\n",
       "9              encoder__x1_Children  -0.34345597567682534"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features_arg = np.flipud(np.argsort(np.abs(log_reg.coef_[0]))[-10:]) # 10 most important features by coefficient\n",
    "important_features_coef = log_reg.coef_[0,important_features_arg] # Their coefficients\n",
    "important_features = pd.DataFrame(c_[feature_names[important_features_arg],important_features_coef],columns=['feature','coefficient']) # Format it nicely\n",
    "print('Features picked out by model {:s}'.format(str(log_reg.__class__)))\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the whole, the logistic regression has identified similar parameters to the random forest, but has some additional insights:\n",
    "1. Filing a joint income tax with both filers under 65 is correlated with a high income.\n",
    "2. Offspring of single mothers are disadvantaged\n",
    "3. Being a child limits your earnings (unsurprisingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was done\n",
    "We loaded the training data and transformed it into a state where a classifier can be trained on it. The solvers used here cannot handle missing data, so as a simple way to account for this the features with missing data were dropped. We removed the *year* feature as this refers to the year the census entry was completed and is unlikely to be predictive of an individual's earning power, and added an additional *highly educated* feature to identify individuals with Bachelors degrees or higher. We also transformed the Male/Female classification and replaced it with a *is male* feature, as otherwise we would have two important colinear features.\n",
    "\n",
    "We applied a *one-hot encoder* to the categorical features of the data, making each possible category a feature in itself to prevent any unphysical ordering to data. Numerical features (e.g. *age*, *wage per hour*, *dividends from stocks*) are passed straight through. We then rescale the data o that each feature has zero mean and unit variance, which often assists with the convergence of the solvers.\n",
    "\n",
    "The learning dataset was split 70%/30% into a training set and a cross-validation set, so that the performance of each solver could be quantified without simply using the training set (which makes the performance evaluation susceptible to overfitting) or using the test set, which should be reserved until the final model is ready.\n",
    "\n",
    "Three classifiers were presented and trained above: a logistic regression; a decision tree; a random forest. The logistic regression and random forest performed very similarly, with the random forest just winning. \n",
    "\n",
    "The random forest was evaluated with the test set, with a score = 95.12%, precision = 72.90%, recall = 38.22%. \n",
    "\n",
    "Analysis of the random forest and linear regression models identified the typical profile of high earners in the US (detailed in the section above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What more could be done\n",
    "Having tried several models, the best model had a score of 95.4%. This doesn't seem that much better than the 93.7% accuracy of a model which assumes nobody earns more than $50k in a year. This could be improved by providing more features, or by training on a much larger dataset. \n",
    "\n",
    "We handled missing data by simply dropping the features with NaN values. It is possible instead to impute the missing values, i.e. to infer them from the known part of the data. This could involve replacing NaN values with the mean (or mode) of that feature, or might use other features to estimate the missing value. I might have implemented this if I could afford to spend the time.\n",
    "\n",
    "We identified Male/Female as colinear features that should be transformed into a single feature in the model. There are additional colinear (or highly correlated) features that could be removed. These can be identified using ``df.corr()`` and identifying pairs with values equal (or close) to 1.0. I implemented this but found it made effectively no difference to the model accuracy, so ultimately removed it from the transformation to keep it simple. Some machine learning methods fail with colinear features, for these such a transformation would be necessary. It is also important to account for colinearity for important features (as we did with *is male*) as it can affect the estimated coefficients.\n",
    "\n",
    "Additional classifiers available in scikit-learn include AdaBoost classifier, Multi-layer Perceptron classifier, and Quadratic discriminant analysis. These were briefly tried (not shown), but not much time was spent trying to optimise these, primarily due to my being less familiar with these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I found challenging\n",
    "This dataset is built significantly around categorical data, such as *education*, *tax filer stat*, or *citizenship*. These features had to be transformed into a meaningful numeric dataset which the models could be trained on, while leaving the noncategorical features unchanged. For this I had to apply a *one-hot encoder*, which I had heard of but not previously implemented.\n",
    "\n",
    "I was hoping to turn this transformation into a pipeline so that the entire model (including transformation and estimation) could be encapsulated into one object. This was complicated by the need to maintain a correspondence between the input feature and resulting transformed feature in the model. Scikit-learn's ``get_feature_names()`` has not yet been implemented for passthrough models, so to implement this in a pipeline-friendly way would require a custom transformation I could not spend the time implementing.\n",
    "\n",
    "I was also unsure of the best way to treat the *num persons worked for employer* feature. This is arguably categorical, as the census technical docs indicate it takes 6 possible values describing the total number of persons who work for their employer, e.g. (0 = nil, 1 = <10, 2 = 10-24, ... , 6 = 1000+). But it also seems inappropriate to treat it with a one-hot encoder, as this data is ordered despite being categorised. In the end I left it with the noncategorical features as it turned out to make very little difference to the models' accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
